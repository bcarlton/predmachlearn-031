---
title: "Predictive Machine Learning Course Project: Classifying Correct and Incorrect Biceps Curls from Inertial Measurement Unit data"
author: "Brian Carlton"
output: html_document
---
```{r setup, echo=FALSE,results='hide'}
suppressWarnings(suppressPackageStartupMessages(library(knitr)))
setwd("~/Desktop/predmachlearn-031/course_project/")
load("summary_tables.RData")
```
NOTE TO READER: This is the complete version with all code included.  For the abridged version, please see [this version]().

The Predictive Machine Learning course offered by Johns Hopkins University/Coursera is a great introduction into the complicated and fascinating machine learning field.  This paper summarizes the investigation into the viability of applying different machine learning approaches to effectively predict the motion of biceps curls given data from Inertial Measurement Units(IMUs) on correct and multiple incorrect examples of the exercise.  The data set being used was collected by researchers in a 2013 published study by the University of Rio de Janeiro's Human Activity Research project, or HAR, investigating a similar concept, albeit in multiple directions and in much more detail.  The full article about their experiment can be found at this [link](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf) and is cited at the end of the paper.

The HAR group captured motion data using 4 IMUs attached to a lifter's belt, arm, forearm and to a 3 lb. dumbbell itself (see Figure 1 for more detail). Then, under the observation of a weight-lifting coach, the subjects performed a biceps curl in 5 different ways: 1 correct way and 4 different incorrect ways.  Each of the lifts by each subject was captured by the IMUs, and all data was assembled into one single dataset. From this dataset, I applied multiple machine learning algorithms to see if I could classify each measurement as a correct lift or as one of the four incorrect lifts. Based on my results, a random forest approach provides the most accuracy given this dataset and should be used for any future classifications in this vein.

Figure 1 
Left: The IMU configuration

Right: It looks like Mr. Schwarzenegger may have benefitted from my random forest model to help his form.  Or, perhaps he just needs to reduce the weight.
![](/home/bc/Desktop/predmachlearn-031/course_project/sensor&arnold.jpg)


###Data Acquisition and Cleanup

The full dataset can be acquired [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv).  Import into the R environment shows that the set consists of just under twenty-thousand observations of one-hundred sixty variables.  The variables are summarized as follows: one outcome variable called "classe", eight bookkeeping variables (user name, timestamp data, windowing labels), forty-eight different raw data variables from the IMUs about euler angles, magnetometer, accelerometer and gyroscope data in each of the four IMUs, and then one-hundred and four summary statistics about the raw data (variances, standard deviations, etc.) on each IMU.

The dataset is in no way clean upon import.  To prepare the data for the learning algorithms, I removed the bookkeeping variables first.  Then, naturally, I checked for missing data. Most of the summary statistics contained missing information.  These statistics are calculated via a sliding window approach, hence the need for the two windowing labels variables, "num window" and "new window".  However, information concerning the details of the sliding windows is not clearly documented, so instead of risking an incorrect mapping of the summary statistics to respective windows, I chose to eliminate those incomplete summary statistics from the data set, eliminating over one-hundred variables in the process.

From the remaining variables, I checked for near-zero variance data, correlated predictors and linear combinations in the observations.  None of the variables were near-zero variance, nor were any of the observations detected as linear combinations.  However, given that the raw IMU measurements were most likely used to calculate the raw Euler angles, some of the variables are probably correlated. So, at a correlation threshold of 0.8, there were a few correlated predictors that were also removed. After the cleaning, the data consisted of 19622 observations of only 40 variables, including the outcome variable.

```{r data_acquisition_and_cleanup, eval=FALSE, results='hide'}
library(doMC)
library(caret)#caret also loads ggplot2 and lattice
library(dplyr)

#set up working environment
registerDoMC(cores=3)
setwd("~/Desktop/predmachlearn-031/course_project/")
set.seed(8675309)## https://www.youtube.com/watch?v=6WTdTwcmxyo

#read in training data first
#reference url: http://groupware.les.inf.puc-rio.br/har
training.url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
training.file.name <- "pml-training.csv"

#downloading file
if(!file.exists(training.file.name)){
  print("File does not exist.")
  download.file(url=training.url, destfile=training.file.name, method="curl")
}
rm(training.url,training.file.name)

#reading data into R
raw <- read.csv("pml-training.csv", header=TRUE, nrows=19700,
                   stringsAsFactors=FALSE, na.strings=c("NA","N/A",""))

#convert outcome variable to factor type
raw$classe <- factor(raw$classe)

#looping through all columns to see which columns are complete
#the variables in which there are blank, #DIV/O and NA variables are unusable due to their inconsistency associated with the window and raw snapshot variables. Start at column 8 to remove the initial 7 bookkeeping variables which have no bearing on the model itself
complete.cols <- vector()
for(i in 8:length(names(raw))){
  if(table(complete.cases(raw[,i]))[1] == 19622){
    complete.cols <- c(complete.cols, i)
  }
}
raw.complete <- raw[ ,complete.cols]
rm(complete.cols)

#splitting outcome from predictors
outcome <- raw.complete$classe
predictors <- raw.complete[,-53]


#checking for near zero variance in predictors
nzvs <- nearZeroVar(x=predictors, saveMetrics=TRUE)$nzv
#selecting those columns which are not nzv type
predictors <- predictors[ ,!nzvs]#none are nzv so no variables are removed
rm(nzvs)


#checking correlations among the variables
cor.predictors <- cor(predictors)
high.corr.pred <- findCorrelation(x=cor.predictors, cutoff=0.8, verbose=TRUE)
#filtering out the highly correlated predictors
predictors <- predictors[ ,-high.corr.pred]#drops from 53 vars to 39
rm(cor.predictors, high.corr.pred)


#checking for and removing linear combinations among the rows
lin.combos <- findLinearCombos(predictors)$remove
if(!is.null(lin.combos)){
  predictors <- predictors[-lin.combos, ]
  outcome <- outcome[-lin.combos]
}
rm(lin.combos)


#reassembling the data into a single frame and removing unnecessary intermediates
processed <- cbind(predictors,outcome)
rm(outcome,predictors,raw.complete)

```


Applying Machine Learning Algorithms
=====================================
As this exercise attempts to identify different classes of performing biceps curls, I used algorithms that were designed mainly for classification purposes and avoided those which are more useful for regression.  Therefore, I avoided methods such as PCA and focused on tree methods (random forest, decision trees), discriminant analysis (linear and quadratic), and some naive bayes and boosting techniques.  These models were developed on a training set, which was a random sample of 60% of the cleaned data.  The remaining 40% of the dataset was used to create a testing set to get an appropriate out-of-sample accuracy estimate.  10-fold cross validation was performed on each of the models in order to strike the right balance between bias and variance.  The data were centered and scaled, but the results in both the tree-based as well as the discriminant analysis based methods were not very different.  The table in Figure 2 shows the out-of-sample results, from which you can clearly determine that the random forest approach is the most effective.

Code for Models:
```{r modeling, eval=FALSE}
#splitting the training and testing sets: 60/40 train/test
in.train <- createDataPartition(y=processed$outcome, p=0.6, list=FALSE)
training.set <- processed[in.train, ]
testing.set <- processed[-in.train, ]
save(raw, processed, training.set, testing.set, file="processed_data.RData")
#load("processed_data.RData")

#MODELS
#======
control <- trainControl(method="repeatedcv",number=10, returnData = FALSE)
pre.proc.obj <- preProcess(x=training.set[,-40], method=c("center","scale"))

norm.training.set <- as.data.frame(cbind(predict(pre.proc.obj, training.set[-40]), outcome=training.set$outcome))
norm.testing.set <- as.data.frame(cbind(predict(pre.proc.obj, testing.set[,-40]), outcome=testing.set$outcome))


#LDA
#===
#simple LDA
simple.lda.model <- train(outcome ~ ., data=training.set, method="lda",
                   trControl=control)
simp.lda.preds <- predict(simple.lda.model, testing.set)
simp.lda.results <- confusionMatrix(simp.lda.preds, testing.set$outcome)
save(simp.lda.preds, simp.lda.results, simple.lda.model,
     file="simpLDA_object.RData")
load("simpLDA_object.RData")

##with scaling
lda.scaled <- train(outcome ~ ., data=norm.training.set, method="lda",
                    trControl=control)
lda.scaled.preds <- predict(lda.scaled, norm.testing.set)
lda.scaled.results <- confusionMatrix(lda.scaled.preds,
                                      norm.testing.set$outcome)

#penalized LDA
penal.lda.model <- train(outcome ~ ., data=training.set,
                         method="PenalizedLDA", trControl=control)
penal.lda.preds <- predict(penal.lda.model, testing.set)
penal.lda.results <- confusionMatrix(penal.lda.preds, testing.set$outcome)
save(penal.lda.model, penal.lda.pred, penal.lda.results,
     file="penalLDA_object.RData")

#stepwise LDA
step.lda.model <- train(outcome ~ ., data=training.set, method='stepLDA',
                        trControl=control)
step.lda.preds <- predict(step.lda.model, testing.set)
step.lda.results <- confusionMatrix(step.lda.preds, testing.set$outcome)
save(step.lda.model, step.lda.preds, step.lda.results,
     file="stepLDA_object.RData")

#QDA
#===============

#simple QDA
simp.qda.model <- train(outcome ~ ., data=training.set, method="qda",
                        trControl=control)
simp.qda.preds <- predict(simp.qda.model, testing.set)
simp.qda.results <- confusionMatrix(simp.qda.preds, testing.set$outcome)
save(simp.qda.model, simp.qda.preds, simp.qda.results,
     file="simpQDA_object.RData")
load("simpQDA_object.RData")

#simp QDA w scaled data
scaled.qda <- train(outcome~., data=norm.training.set, method="qda",
                    trControl=control)
scaled.qda.preds <- predict(scaled.qda, norm.testing.set)
scaled.qda.results <- confusionMatrix(data=scaled.qda.preds,
                                      reference=norm.testing.set$outcome)

#stepwise QDA
step.qda.model <- train(outcome ~ ., data=training.set, method='stepQDA',
                        trControl=control)
step.qda.preds <- predict(step.qda.model, testing.set)
step.qda.results <- confusionMatrix(step.qda.preds, testing.set$outcome)
save(step.qda.model, step.qda.preds, step.qda.results,
     file="stepQDA_object.RData")

#NAIVE BAYES
#===========
nb.model <- train(outcome ~ ., data=training.set, method="nb",
                  trControl=control)
nb.preds <- predict(nb.model, testing.set)
nb.results <- confusionMatrix(nb.preds, testing.set$outcome)
save(nb.model, nb.preds, nb.results, file="nb_object.RData")

#BOOSTING
#========
#stochastic gradient boosting (gbm)
gbm.model <- train(outcome ~., data=training.set, method="gbm",
                   trControl=control)
gbm.preds <- predict(gbm.model, testing.set)
gbm.results <- confusionMatrix(gbm.preds, testing.set$outcome)
save(gbm.model, gbm.preds, gbm.results,  file="gbm_object.RData")


#logitboost model
logitboost.model <- train(outcome~., data=training.set,
                          method="LogitBoost",trControl=control)
logitboost.preds <- predict(logitboost.model, testing.set)
logitboost.results <- confusionMatrix(logitboost.preds, testing.set$outcome)
save(logitboost.model, logitboost.preds, logitboost.results,
     file="logitboost_object.RData")

#STANDARD DECISION TREE
#======================
#standard Tree model with rpart
tree.model <- train(outcome~., data=training.set, method="rpart",
                    trControl=control)
tree.preds <- predict(tree.model, testing.set)
tree.results <- confusionMatrix(tree.preds, testing.set$outcome)
save(tree.model, tree.preds, tree.results, file="tree_object.RData")

#RANDOM FOREST
#=============

#all predictors
rf.model.all <- train(outcome ~ ., data=training.set, proximity=TRUE, trControl=control, importance=TRUE)

#importance for all predictor model
rf.importance <- varImp(rf.model.all)
rf.all.imp.plot <- plot(rf.importance, main="Random Forest All Predictors Importance")
png(filename="rf_all_importance_plot.png", width=1159, height=726, units="px")
rf.all.imp.plot
dev.off()

#predictions and results for all predictor model
rf.all.preds <- predict(rf.model.all, testing.set)
rf.all.results <- confusionMatrix(rf.all.preds, testing.set$outcome)

save(rf.model.all, rf.all.imp.plot, rf.all.preds, rf.all.results, rf.importance,
     file="rfmodel_allpreds.RData")
#load("rfmodel_allpreds.RData")

#creating results table
results.table <- rbind(
  gbm.results$overall[1:4],
  logitboost.results$overall[1:4],
  nb.results$overall[1:4],
  penal.lda.results$overall[1:4],
  rf.all.results$overall[1:4],
  simp.lda.results$overall[1:4],
  simp.qda.results$overall[1:4],
  step.lda.results$overall[1:4],
  step.qda.results$overall[1:4],
  tree.results$overall[1:4]
  )

results.table <- as.data.frame(results.table)
results.table <- select(results.table, -Kappa)
methods <- c("Stochastic Gradient Boosting",
                             "LogitBoost",
                             "Naive Bayes",
                             "Penalized LDA",
                             "Random Forest",
                             "Standard LDA",
                             "Standard QDA",
                             "Stepwise LDA",
                             "Stepwise QDA",
                             "Decision Tree")

results.table <- cbind(Methods=methods, results.table)
results.table <- arrange(results.table, desc(Accuracy))
names(results.table) <- c("Method", "Accuracy", "Lower Bound", "Upper Bound")
```


Figure 2: Out-of-sample summary.  The Upper and Lower Bounds represent the 95% accuracy confidence interval.
```{r results_table,echo=FALSE}
kable(results.table)
```


###Tweaking the Random Forest
The Random Forest machine learning algorithm is highly effective and, naturally, has a reputation for being one of the most effective learning approaches with very little adjusting needed.  The fact that it randomly samples predictors at each split avoids the problem of highly correlated trees that may occur with strong predictors in a bagging approach.  However, this effectiveness comes at the cost of interpretability and simplicity.  These factors are important, so I attempted to take Occam's Razor to this model, if you will, to see if I could not simplify the model using only the most important predictors.  From the random forest with all predictors, I calculated variable importance from the Gini index, and tried to develop models using smaller subsets of those variables.  Using the top five and ten variables did not yield enough variability for the random forest approach to be generated, but using the top 15 did.  Furthermore, reducing to only 15 predictor variables allowed for a smaller model without sacrificing much accuracy. Here are the accuracy comparisons as well as the importance plots for the original model and the reduced model.

Code for Top 15 Model:
```{r top15, results='asis', eval=FALSE}
#top 15
#top 15 values were taken directly from the importance table from the complete rf model
top15.formula <- formula(outcome~yaw_belt + pitch_forearm + magnet_dumbbell_z+magnet_dumbbell_y + gyros_dumbbell_y + magnet_belt_z+magnet_belt_y + roll_arm + gyros_belt_z + roll_forearm + pitch_arm + accel_forearm_x + total_accel_dumbbell + gyros_arm_y + yaw_arm)

rf.model.top15 <- train(top15.formula, data=training.set, proximity=T,
                        trControl=control, importance=T)

#importance
rf.top15.imp.plot <- plot(varImp(rf.model.top15), main="Random Forest Top 15 Importance")
png(filename="rf_top15_importance_plot.png", width=1159, height=726, units="px")
rf.top15.imp.plot
dev.off()

#predictions/results
rf.top15.preds <- predict(rf.model.top15, testing.set)
rf.top15.results <- confusionMatrix(rf.top15.preds, testing.set$outcome)

save(rf.model.top15, rf.top15.imp.plot, rf.top15.preds, rf.top15.results,
     file="rfmodel_Top15.RData")
#load("rfmodel_Top15.RData")

#creating comparison table for random forests
rf.table <- rbind(
  rf.all.results$overall[1:4],
  rf.top15.results$overall[1:4]
  )
rf.table <- as.data.frame(rf.table)
rf.table <- select(rf.table, -Kappa)
rf.table <- cbind(
  Type=c("All Predictors", "Top 15 Predictors"),
  rf.table
  )
save(rf.table, results.table, file="summary_tables.RData")
```


Figure 3: Comparison of Accuracy between Random Forest Models
```{r rf_comparison, echo=FALSE}
kable(rf.table)
```

Figure 4: Importance Plots for each Random Forest Model
![](/home/bc/Desktop/predmachlearn-031/course_project/rf_all_importance_plot.png)

![](/home/bc/Desktop/predmachlearn-031/course_project/rf_top15_importance_plot.png)

The difference in accuracies is extremely small, meriting the use of the Top 15 model over the full prediction model to save computational cost. Concerning importance, the "yaw belt" category remained at the top of the importance list in both models, and two of the magnetometer readings on the dumbbell remained highly important as well, while the "pitch forearm" category dropped slightly. This information may be useful in moving toward a less complicated approach using fewer sensors.

###Caveats and Moving Forward
There are a few topics for future analysis.

* Original dataset sampling windows:  it is bothersome to dismiss the approach that the original researchers took in applying a machine learning algorithm.  From each of the raw data snapshots from the IMUs, summary statistics were calculated, and then the researchers used a [correlated feature selection algorithm](http://www.cs.waikato.ac.nz/~mhall/thesis.pdf) to select 18 of these statistics on which to make a random forest model.  This is in direct contradiction to the methods I used, in which most of the model is based on the combinations of raw data measurements.  It is very natural to suggest examining models based on combinations of raw data and summary statistics and not just one set or the other, while using different feature selection approaches.

* Modeling on single IMU measurements: It would be interesting to see how well a single IMU can predict the class of biceps curl.  It is unlikely that any automated system that would identify errors in lifting technique might be constrained to a single device, such as a smartphone.  Thus, being able to predict on a single location is much more feasible for realizing a practical, machine assisted coach, and so the potential of light resource machine learning algorithms should be investigated. Using different models such as in the case of the random forests could point to a best use location, such as on a smartphone on a belt or a small sensor on a dumbbell directly.

###Citations and Thanks

For the entire work done by the University of Rio de Janeiro's HAR team, please consult the following citation:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

The paper can be found directly at this link: http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf

All files and R objects associated with this project can be found at my GitHub repository:
https://github.com/bcarlton/predmachlearn-031.git

Thank you all for reading!
